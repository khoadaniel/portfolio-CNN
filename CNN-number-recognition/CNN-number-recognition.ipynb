{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs / ConvNets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "<br>\n",
    "https://sci2lab.github.io/ml_tutorial/cnn/\n",
    "<br>\n",
    "https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\n",
    "<br>\n",
    "https://machinelearningmastery.com/deep-learning-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every object must be flattened and then represented with a vector, that will be the input vector for training.\n",
    "<br> Every class in the dataset must be represented with its one-hot label vector. A one-hot label vector is a vector with size equal to the number of different classes in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword\n",
    "<br> element-wise matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digit Recognition using Convolutional Neural Networks in Python with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load (downloaded if needed) the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# plot 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#the data set has been split already 70k:10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape((X_train.shape[0], num_pixels)).astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0], num_pixels)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape \n",
    "#X_train is a 3D array with shape[0] as the number of images\n",
    "#before flatteing, it was (60000,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train is an array of 0 to 9\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n",
    "#now we have 10 dummy columns to denote 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 784)               615440    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                7850      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 623,290\n",
      "Trainable params: 623,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "\t#the first ladyer will take input_dim as the dimension of input\n",
    "\tmodel.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "\t\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here is an example of how softmax() works:\n",
    "<br># define data\n",
    "<br>data = [1, 3, 2]\n",
    "<br># calculate softmax\n",
    "<br>result = softmax(data)\n",
    "<br># report the probabilities\n",
    "<br>print(result)\n",
    "<br># report the sum of the probabilities\n",
    "<br>print(sum(result))\n",
    "<br>----> [0.09003057 0.66524096 0.24472847]\n",
    "<br>----> 0.9999999999999997"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a simple neural network with one hidden layer with the same number of neurons as there are inputs (784). A rectifier activation function is used for the neurons in the hidden layer.\n",
    "\n",
    "A softmax activation function is used on the output layer to turn the outputs into probability-like values and allow one class of the 10 to be selected as the model’s output prediction. Logarithmic loss is used as the loss function (called categorical_crossentropy in Keras) and the efficient ADAM gradient descent algorithm is used to learn the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very interesting on how the model accruacy is calcualted. Accruacy should be calcualted with (y_true;y_pred) when y_pred denote the class with the highest probablility. Refer to this thread:\n",
    "<br>\n",
    "https://towardsdatascience.com/softmax-regression-in-python-multi-class-classification-3cb560d90cb2\n",
    "<br>\n",
    "https://stackoverflow.com/questions/60387266/how-do-we-calculate-the-accuracy-of-a-multi-class-classifier-using-neural-network\n",
    "<br>\n",
    "https://stats.stackexchange.com/questions/517930/metrics-for-multiclass-classification-model-accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 - 2s - loss: 0.2805 - accuracy: 0.9214 - val_loss: 0.1372 - val_accuracy: 0.9589 - 2s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "300/300 - 1s - loss: 0.1095 - accuracy: 0.9676 - val_loss: 0.0928 - val_accuracy: 0.9720 - 1s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "300/300 - 1s - loss: 0.0710 - accuracy: 0.9795 - val_loss: 0.0849 - val_accuracy: 0.9728 - 1s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "300/300 - 1s - loss: 0.0497 - accuracy: 0.9855 - val_loss: 0.0658 - val_accuracy: 0.9797 - 1s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "300/300 - 1s - loss: 0.0364 - accuracy: 0.9899 - val_loss: 0.0645 - val_accuracy: 0.9788 - 1s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "300/300 - 1s - loss: 0.0270 - accuracy: 0.9926 - val_loss: 0.0679 - val_accuracy: 0.9786 - 1s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "300/300 - 1s - loss: 0.0196 - accuracy: 0.9949 - val_loss: 0.0568 - val_accuracy: 0.9824 - 1s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "300/300 - 1s - loss: 0.0140 - accuracy: 0.9969 - val_loss: 0.0592 - val_accuracy: 0.9820 - 1s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "300/300 - 1s - loss: 0.0110 - accuracy: 0.9979 - val_loss: 0.0592 - val_accuracy: 0.9835 - 1s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "300/300 - 1s - loss: 0.0080 - accuracy: 0.9986 - val_loss: 0.0572 - val_accuracy: 0.9840 - 1s/epoch - 4ms/step\n",
      "Baseline Error: 1.60%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit and evaluate the model. The model is fit over 10 epochs with updates every 200 images. The test data is used as the validation dataset, allowing you to see the skill of the model as it trains. A verbose value of 2 is used to reduce the output to one line for each training epoch.\n",
    "<br>\n",
    "This means that batch_size=200 (go through 200 images before updating the model)\n",
    "<br>\n",
    "Epoch = 10 means go through the entire dataset 10 times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 1.60%\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][width][height][channels]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below summarizes the network architecture.\n",
    "\n",
    "<br> The first hidden layer is a convolutional layer called a Convolution2D. The layer has 32 feature maps, which with the size of 5×5 and a rectifier activation function. This is the input layer, expecting images with the structure outline above [pixels][width][height].\n",
    "\n",
    "<br> Next we define a pooling layer that takes the max called MaxPooling2D. It is configured with a pool size of 2×2.\n",
    "\n",
    "<br> The next layer is a regularization layer using dropout called Dropout. It is configured to randomly exclude 20% of neurons in the layer in order to reduce overfitting.\n",
    "\n",
    "<br> Next is a layer that converts the 2D matrix data to a vector called Flatten. It allows the output to be processed by standard fully connected layers. Read more here: https://towardsdatascience.com/the-most-intuitive-and-easiest-guide-for-convolutional-neural-network-3607be47480\n",
    "\n",
    "<br> Next a fully connected layer with 128 neurons and rectifier activation function.\n",
    "\n",
    "<br> Finally, the output layer has 10 neurons for the 10 classes and a softmax activation function to output probability-like predictions for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 24, 24, 32)        832       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 12, 32)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               589952    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 592,074\n",
      "Trainable params: 592,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "\t\t#there are 32 activation maps produced by using 5x5 features on the input\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2))) #there are \n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\t\t#here, the flattener will acutallt flatten the previous layers into 1 array (althought befor they were several maps)\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "300/300 - 9s - loss: 0.2417 - accuracy: 0.9309 - val_loss: 0.0750 - val_accuracy: 0.9767 - 9s/epoch - 29ms/step\n",
      "Epoch 2/2\n",
      "300/300 - 9s - loss: 0.0733 - accuracy: 0.9778 - val_loss: 0.0507 - val_accuracy: 0.9830 - 9s/epoch - 29ms/step\n",
      "CNN Error: 1.70%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "predict_result = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.4077546e-10, 1.4370258e-10, 6.1811811e-08, 2.1913081e-07,\n",
       "       8.4582695e-11, 2.0672311e-09, 8.2988598e-16, 9.9999928e-01,\n",
       "       4.9324655e-09, 3.5621571e-07], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa4fd7352e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[0], cmap=plt.get_cmap('gray'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is Verbose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a formatted number 1234.57%\n"
     ]
    }
   ],
   "source": [
    "number = 1234.5678\n",
    "print('This is a formatted number %.2f%%'%number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 32)        832       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 32)        0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               589952    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 592,074\n",
      "Trainable params: 592,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def basline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5,5), input_shape=(28,28,1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_corssentropy', optimizer='adam', metrics=['accurary'])\n",
    "    return model\n",
    "\n",
    "#build model\n",
    "model = baseline_model()\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3aeb15c8c31224d9ef37a76c0046f703a279f439b6efd04fb2681e5f2715bf2f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
